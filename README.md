#  Notes of Speech and Audio

1. 听觉的掩蔽效应

   https://blog.csdn.net/kwenzhang1/article/details/142552558

   作用：让编解码器中过程中产生的量化失真低于掩蔽阈值，这样这些失真人耳就感知不到了。

2. 语谱图

   顺序：分帧加窗->短时傅里叶变换->频谱图逆时针旋转90°并对幅度值进行映射，通过量化的方式，0表示白，255表示黑色。幅度值越大，相应的区域越黑， 从而去除了幅度值，这个维度， 多出一个维度用作表达其他信息->时间作横轴，频率作纵轴形成语谱图。

   参考链接：https://blog.csdn.net/chumingqian/article/details/123019808

   https://blog.csdn.net/qq_36002089/article/details/108378796

3. 梅尔谱图

   语谱图往往是很大的一张图，为了得到合适大小的声音特征，往往把它通过梅尔标度滤波器组（mel-scale filter banks），变换为梅尔频谱。
   人耳听到的声音高低和实际（Hz）频率不呈线性关系，用Mel频率更符合人耳的听觉特性（这正是用Mel声谱图的一个动机，由人耳听力系统启发），即在1000Hz以下呈线性分布，1000Hz以上呈对数增长，Mel频率与Hz频率的关系为：
   fmel=2595⋅lg(1+f/700Hz)。
   有另一种计算方式为fmel=1125⋅ln(1+f700Hz)。

4. 倒谱

   频谱平方->功率谱->对数运算->逆傅里叶变换->倒谱

   上述步骤如果没有对数运算，可以得到自相关序列

​	参考链接：https://www.cnblogs.com/dream-and-truth/p/10655758.html

# Notes of ANN

### 1.ReLU函数

全称为Rectified Linear Unit（修正线性单元），是一种在深度学习中常用的激活函数。它的作用是将输入值进行非线性转换，以便神经网络能够学习和模拟复杂的模式和关系。

ReLU函数的定义非常简单：对于任何输入值 x*x*，如果 x*x* 大于0，ReLU函数就输出 x*x* 本身；如果 x*x* 小于或等于0，ReLU函数就输出0。用数学公式表示就是：

ReLU(x)=max⁡(0,x)ReLU(*x*)=max(0,*x*)

这个函数之所以受欢迎，主要有以下几点原因：

1. **计算简单**：ReLU函数的计算非常简单，只需要判断输入值是否大于0，然后输出相应的结果。这种简单的计算使得ReLU函数在大规模神经网络中运行得更快，效率更高。
2. **缓解梯度消失问题**：在深度神经网络中，梯度消失是一个常见问题，即在反向传播过程中，梯度会逐渐变小，导致网络的深层部分难以训练。ReLU函数在 x>0时的导数为1，这意味着在正数部分，梯度不会消失，从而缓解了梯度消失问题，使得深层网络更容易训练.
3. **稀疏激活**：由于ReLU函数会将所有小于或等于0的输入值输出为0，因此在实际应用中，神经网络的很多神经元在某一时刻可能不会被激活（即输出为0）。这种稀疏激活特性可以减少神经网络的复杂性和计算量，同时也有助于提高模型的泛化能力

### 2.偏置与权重

权重（Weights）

- **作用**：权重决定了输入数据在神经网络中的重要性。每个输入特征与一个权重相乘，权重越大，该特征对最终输出的影响就越大。
- **类比**：可以将其想象成在决策过程中，不同因素的“重要程度”。比如在一个简单的决策中，你可能会考虑多个因素（如价格、品牌、功能等），每个因素都有一个权重，表示它在你决策中的重要性。
- **调整过程**：在训练过程中，神经网络通过调整权重来学习数据中的模式和关系。权重的调整是通过反向传播算法实现的，该算法根据损失函数的梯度来更新权重，使得模型的预测更接近实际值.

偏置（Biases）

- **作用**：偏置是一个常数项，它允许神经网络在没有输入的情况下也能产生输出。偏置的存在使得神经网络的输出不完全依赖于输入数据，而是可以有一个“基础值”.
- **类比**：可以将其想象成在决策中的“起始点”或“默认值”。比如在一个决策中，即使你没有考虑任何因素，你可能也会有一个默认的选择或倾向.
- **调整过程**：与权重类似，偏置也会在训练过程中被调整。通过反向传播算法，偏置的值会根据损失函数的梯度进行更新，以帮助模型更好地拟合数据

### 3.神经网络各层

输入层（Input Layer）

- **作用**：接收输入数据。
- **通俗解释**：就像一个接收站，它把原始数据（如图片、文本、数值等）送入神经网络进行处理。输入层的节点数通常与输入数据的特征数相匹配，每个节点对应一个特征.

隐藏层（Hidden Layers）

- **作用**：提取特征和进行非线性变换。
- **通俗解释**：隐藏层是神经网络的“大脑”，负责从输入数据中提取有用的特征和模式。每一层都会对前一层的输出进行处理，逐步抽象和总结出更高级的信息。隐藏层的数量和每层的节点数可以根据具体问题进行调整，以达到更好的学习效果。隐藏层的节点通过权重和偏置与前一层节点相连，通过激活函数（如ReLU）进行非线性变换，使得神经网络能够学习复杂的模式.

输出层（Output Layer）

- **作用**：生成最终的预测结果。
- **通俗解释**：输出层是神经网络的“出口”，它将隐藏层处理后的信息转换为最终的输出结果。输出层的节点数通常与任务的输出维度相匹配。例如，在分类任务中，输出层的节点数等于类别数，每个节点的输出值表示对应类别的概率或置信度；在回归任务中，输出层通常只有一个节点，输出一个连续的数值作为预测结果.

卷积层（Convolutional Layers）

- **作用**：提取局部特征和模式。
- **通俗解释**：卷积层主要用于处理图像数据，它通过卷积操作在输入数据上滑动一个小的滤波器（卷积核），提取局部区域的特征。卷积操作可以捕捉到图像中的边缘、纹理、形状等局部信息，使得神经网络能够更好地理解和识别图像内容。卷积层具有参数共享的特性，即同一个卷积核在整个输入数据上重复使用，这大大减少了参数数量，提高了计算效率.

池化层（Pooling Layers）

- **作用**：降采样和提取主要特征。
- **通俗解释**：池化层通常紧跟在卷积层之后，它的作用是降低数据的空间维度，减少计算量和参数数量，同时保留主要的特征信息。常见的池化操作有最大池化和平均池化。最大池化取局部区域的最大值，可以突出显著的特征；平均池化取局部区域的平均值，可以平滑特征。池化层有助于提高模型的鲁棒性，使模型对输入数据的平移、缩放等变化具有一定的不变性.

全连接层（Fully Connected Layers）

- **作用**：整合特征并进行分类或回归。
- **通俗解释**：全连接层是神经网络中的一种常见层，它的每个节点都与前一层的所有节点相连。全连接层通常用于神经网络的最后几层，负责整合前面各层提取的特征信息，并进行分类或回归任务。在分类任务中，全连接层的输出节点数等于类别数，通过激活函数（如softmax）将输出转换为概率分布；在回归任务中，全连接层输出一个连续的数值作为预测结果.

Dropout层（Dropout Layers）

- **作用**：防止过拟合。
- **通俗解释**：Dropout层是一种正则化技术，用于防止神经网络在训练过程中过拟合。在训练时，Dropout层会随机“丢弃”（即置为零）一定比例的节点及其连接，使得网络的每个节点不能过分依赖于其他节点，从而增强模型的泛化能力。在测试阶段，Dropout层通常不使用，或者以某种方式调整节点的输出值，以保持模型的整体性能.

批归一化层（Batch Normalization Layers）

- **作用**：加速训练并提高模型稳定性。
- **通俗解释**：批归一化层通过对每个小批量数据进行归一化处理，使得网络中各层的输入分布更加稳定。具体来说，它会将每个特征的均值调整为0，方差调整为1，从而减少“内部协变量偏移”现象。这有助于加速神经网络的训练过程，使得模型能够使用更高的学习率，并提高模型的稳定性和性能.